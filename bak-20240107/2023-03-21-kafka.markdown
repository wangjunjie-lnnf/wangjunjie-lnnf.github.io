---
layout: post
title:  "kafka"
date:   2023-03-21 19:22:07 +0000
categories: jekyll
tags: kafka
---

# kafka

## boot

```scala
def main(args: Array[String]): Unit = {
    val serverProps = getPropsFromArgs(args)
    {
        val props = Utils.loadProps(args(0))
    }

    val server = buildServer(serverProps)
    {
        val config = KafkaConfig.fromProps(props, false)
    
        // process.roles参数决定元数据存储是zk或kraft
        if (config.requiresZookeeper) {
            new KafkaServer(
                config,
                Time.SYSTEM,
                threadNamePrefix = None,
                enableForwarding = false
            )
        } else {
            new KafkaRaftServer(
                config,
                Time.SYSTEM,
                threadNamePrefix = None
            )
        }
    }

    server.startup()
    {
        // 和zk建立连接, 创建目录
        initZkClient(time)

        /* 从zk获取topic(/topics/xxx)或broker(/brokers/xxx)的配置 */
        configRepository = new ZkConfigRepository(new AdminZkClient(zkClient))

        /* zk路径: /cluster/id */
        _clusterId = getOrGenerateClusterId(zkClient)

        // 加载meta.properties, initialOfflineDirs为不包含meta.properties的dir
        val (preloadedBrokerMetadataCheckpoint, initialOfflineDirs) =
          BrokerMetadataCheckpoint.getBrokerMetadataAndOfflineDirs(config.logDirs, ignoreMissing = true)

        // 同步zk中的配置/brokers/{brokerId}和/brokers/<default>
        config.dynamicConfig.initialize(Some(zkClient))

        /* 基于ScheduledThreadPoolExecutor，调度定时任务 */
        kafkaScheduler = new KafkaScheduler(config.backgroundThreads)
        kafkaScheduler.startup()

        _logManager = LogManager(config, initialOfflineDirs, ...)
        logManager.startup(zkClient.getAllTopicsInCluster())
        {
            // 数据恢复
            loadLogs(defaultConfig, topicConfigOverrides)

            scheduler.schedule("kafka-log-retention", cleanupLogs _, ...)
            scheduler.schedule("kafka-log-flusher", flushDirtyLogs _, ...)
            scheduler.schedule("kafka-recovery-point-checkpoint", checkpointLogRecoveryOffsets _, ...)
            scheduler.schedule("kafka-log-start-offset-checkpoint", checkpointLogStartOffsets _, ...)
            scheduler.schedule("kafka-delete-logs", deleteLogs _, ...)

            if (cleanerConfig.enableCleaner) {
                _cleaner = new LogCleaner(cleanerConfig, liveLogDirs, currentLogs, logDirFailureChannel, time = time)
                _cleaner.startup()
            }
        }
        
        // 维护每个分区的状态，由controller发送更新通知
        metadataCache = MetadataCache.zkMetadataCache(config.brokerId, ...)

        // 缓存并发送通过sendRequest(...)发送的请求
        clientToControllerChannelManager = BrokerToControllerChannelManager(controllerNodeProvider = controllerNodeProvider, ...)
        clientToControllerChannelManager.start()
        
        socketServer = new SocketServer(config, ...);
        {
            // 接受client连接
            val dataPlaneAcceptors = new ConcurrentHashMap[EndPoint, DataPlaneAcceptor]()
            // 请求队列
            val dataPlaneRequestChannel = new RequestChannel(maxQueuedRequests, ...)

            // 每个监听地址创建一个acceptor以及一组io线程和一个请求队列
            config.dataPlaneListeners.foreach(createDataPlaneAcceptorAndProcessors)
            {
                val dataPlaneAcceptor = createDataPlaneAcceptor(endpoint, ..., dataPlaneRequestChannel)
                {
                    val serverChannel = openServerSocket(endPoint.host, endPoint.port, listenBacklogSize)
                    val thread = KafkaThread.nonDaemon(this)
                    processors.foreach(_.start())
                    thread.start()
                }

                dataPlaneAcceptor.configure(parsedConfigs)
                {
                    addProcessors(configs.get(KafkaConfig.NumNetworkThreadsProp).asInstanceOf[Int])
                    {
                        for (_ <- 0 until toCreate) {
                            val processor = newProcessor(socketServer.nextProcessorId(), ...)
                            // listenerProcessors用于处理新的client连接
                            listenerProcessors += processor
                            // requestChannel用于请求队列、把响应信息加入到processor的响应队列
                            requestChannel.addProcessor(processor)
                        }
                    }
                }
                dataPlaneAcceptors.put(endpoint, dataPlaneAcceptor)
            }
        }

        _replicaManager = createReplicaManager(isShuttingDown)
        replicaManager.startup()
        {
            scheduler.schedule("isr-expiration", maybeShrinkIsr _, ...)
            scheduler.schedule("shutdown-idle-replica-alter-log-dirs-thread", shutdownIdleReplicaAlterLogDirsThread _, ...)
            logDirFailureHandler = new LogDirFailureHandler("LogDirFailureHandler", haltBrokerOnFailure)
            logDirFailureHandler.start()
        }

        val brokerInfo = createBrokerInfo
        val brokerEpoch = zkClient.registerBroker(brokerInfo)

        // 处理各种event
        _kafkaController = new KafkaController(config, zkClient, ...)
        kafkaController.startup()
        {
            eventManager.put(Startup)
            eventManager.start()
            {
                dequeued.process(processor);
                {
                    processor.process(event)
                    {
                        event match {
                            case ReplicaLeaderElection(partitions, electionType, electionTrigger, callback) =>
                                processReplicaLeaderElection(partitions, electionType, electionTrigger, callback)
                            case LeaderAndIsrResponseReceived(response, brokerId) =>
                                processLeaderAndIsrResponseReceived(response, brokerId)
                        }
                    }
                }
            }
        }

        groupCoordinator = GroupCoordinator(config, replicaManager, ...)
        groupCoordinator.startup(() => zkClient.getTopicPartitionCount(Topic.GROUP_METADATA_TOPIC_NAME)
                                               .getOrElse(config.offsetsTopicPartitions))

        val fetchManager = new FetchManager(...)

        // 请求处理线程
        dataPlaneRequestProcessor = createKafkaApis(socketServer.dataPlaneRequestChannel)
        dataPlaneRequestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, 
                    socketServer.dataPlaneRequestChannel, dataPlaneRequestProcessor, config.numIoThreads, ...)

        // 认证成功之后开始接受连接
        socketServer.enableRequestProcessing(...)
        _brokerState = BrokerState.RUNNING
    }

    server.awaitShutdown()
}
```

## 请求处理

![kafka-network](/assets/images/2023-03-21/kafka-net.png)

```scala

// acceptor线程
class DataPlaneAcceptor(...) {

    val nioSelector = NSelector.open()
    val serverChannel = openServerSocket(endPoint.host, endPoint.port, listenBacklogSize)

    override def run(): Unit = {
        // 注册OP_ACCEPT事件
        serverChannel.register(nioSelector, SelectionKey.OP_ACCEPT)
        
        while (shouldRun.get()) {
            acceptNewConnections()
            {
                nioSelector.select(500)
                val keys = nioSelector.selectedKeys()
                val iter = keys.iterator()
                while (iter.hasNext && shouldRun.get()) {
                    if (key.isAcceptable) {
                        accept(key).foreach { socketChannel =>
                            do {
                                // 轮流使用processor线程
                                processor = synchronized {
                                    currentProcessorIndex = currentProcessorIndex % processors.length
                                    processors(currentProcessorIndex)
                                }
                                // 由processor处理
                            } while (!assignNewConnection(socketChannel, processor, ...))
                        }
                    }
                }
            }

            closeThrottledConnections()
        }
    }
}

// 网络IO线程
class Processor(...) {

    override def run(): Unit = {
        while (shouldRun.get()) {
            // 注册请求socket的OP_READ事件到selector
            configureNewConnections()
            {
                selector.register(connectionId(channel.socket), channel)
                {
                    registerChannel(id, socketChannel, SelectionKey.OP_READ);
                }
            }

            processNewResponses()
            {
                while ({currentResponse = dequeueResponse(); currentResponse != null}) {
                    currentResponse match {
                        case response: SendResponse =>
                            sendResponse(response, response.responseSend)
                            {
                                selector.send(new NetworkSend(connectionId, responseSend))
                                {
                                    String connectionId = send.destinationId();
                                    KafkaChannel channel = openOrClosingChannelOrFail(connectionId);
                                    channel.setSend(send);
                                    {
                                        this.transportLayer.addInterestOps(SelectionKey.OP_WRITE);
                                    }
                                }
                            }
                    }
                }
            }

            // 读请求到completedReceives，发送写请求
            poll()
            {
                selector.poll(pollTimeout)
                {
                    nioSelector.select(timeoutMs);
                    pollSelectionKeys(readyKeys, false, endSelect);
                    {
                        // 一次只读取一个请求
                        if (channel.ready() 
                            && (key.isReadable() || channel.hasBytesBuffered()) 
                            && !hasCompletedReceive(channel)) {
                            attemptRead(channel);
                            {
                                long bytesReceived = channel.read();
                                if (bytesReceived != 0) {
                                    // 等待一个完整的数据包接收完成，前4字节表示数据包长度
                                    // 完整的数据包放入completedReceives
                                    NetworkReceive receive = channel.maybeCompleteReceive();
                                    if (receive != null) {
                                        addToCompletedReceives(channel, receive, currentTimeMs);
                                    }
                                }
                            }
                        }

                        // 发送数据
                        attemptWrite(key, channel, nowNanos);
                        {
                            if (channel.hasSend() && channel.ready() && key.isWritable()) {
                                write(channel);
                            }
                        }
                    }
                }
            }

            // 把completedReceives上的请求发送到requestChannel
            processCompletedReceives()
            {
                selector.completedReceives.forEach { receive =>
                    openOrClosingChannel(receive.source) match {
                        case Some(channel) =>
                            val header = parseRequestHeader(receive.payload)
                            val context = new RequestContext(header, connectionId, ...)
                            val req = new RequestChannel.Request(processor = id, context = context, ...)
                            // 接收完成的请求放到requestChannel
                            requestChannel.sendRequest(req)
                    }
                }
            }

            // respond发送完成调用onComplete
            processCompletedSends()
            {
                selector.completedSends.forEach { send =>
                    val response = inflightResponses.remove(send.destinationId)
                    response.onComplete.foreach(onComplete => onComplete(send))
                }
            }

            processDisconnected()
            closeExcessConnections()
        }
    }

}

// 请求处理线程
class KafkaRequestHandler(...) {

    def run(): Unit = {
        while (!stopped) {
            val req = requestChannel.receiveRequest(300)
            req match {
                case request: RequestChannel.Request =>
                    apis.handle(request, requestLocal)
                    {
                        request.header.apiKey match {
                            case ApiKeys.PRODUCE => handleProduceRequest(request, requestLocal)
                            case ApiKeys.FETCH => handleFetchRequest(request)
                            case ApiKeys.LIST_OFFSETS => handleListOffsetRequest(request)
                            case ApiKeys.METADATA => handleTopicMetadataRequest(request)
                            case ApiKeys.LEADER_AND_ISR => handleLeaderAndIsrRequest(request)
                            case ApiKeys.STOP_REPLICA => handleStopReplicaRequest(request)
                            case ApiKeys.UPDATE_METADATA => handleUpdateMetadataRequest(request, requestLocal)
                            case ApiKeys.CONTROLLED_SHUTDOWN => handleControlledShutdownRequest(request)
                            case ApiKeys.OFFSET_COMMIT => handleOffsetCommitRequest(request, requestLocal)
                            case ApiKeys.OFFSET_FETCH => handleOffsetFetchRequest(request)
                            case ApiKeys.FIND_COORDINATOR => handleFindCoordinatorRequest(request)
                            case ApiKeys.JOIN_GROUP => handleJoinGroupRequest(request, requestLocal)
                            case ApiKeys.HEARTBEAT => handleHeartbeatRequest(request)
                            case ApiKeys.LEAVE_GROUP => handleLeaveGroupRequest(request)
                            case ApiKeys.SYNC_GROUP => handleSyncGroupRequest(request, requestLocal)
                            case ApiKeys.DESCRIBE_GROUPS => handleDescribeGroupRequest(request)
                            case ApiKeys.LIST_GROUPS => handleListGroupsRequest(request)
                            case ApiKeys.SASL_HANDSHAKE => handleSaslHandshakeRequest(request)
                            case ApiKeys.API_VERSIONS => handleApiVersionsRequest(request)
                            case ApiKeys.CREATE_TOPICS => maybeForwardToController(request, handleCreateTopicsRequest)
                            case ApiKeys.DELETE_TOPICS => maybeForwardToController(request, handleDeleteTopicsRequest)
                            case ApiKeys.DELETE_RECORDS => handleDeleteRecordsRequest(request)
                            case ApiKeys.INIT_PRODUCER_ID => handleInitProducerIdRequest(request, requestLocal)
                            case ApiKeys.OFFSET_FOR_LEADER_EPOCH => handleOffsetForLeaderEpochRequest(request)
                            case ApiKeys.ADD_PARTITIONS_TO_TXN => handleAddPartitionToTxnRequest(request, requestLocal)
                            case ApiKeys.ADD_OFFSETS_TO_TXN => handleAddOffsetsToTxnRequest(request, requestLocal)
                            case ApiKeys.END_TXN => handleEndTxnRequest(request, requestLocal)
                            case ApiKeys.WRITE_TXN_MARKERS => handleWriteTxnMarkersRequest(request, requestLocal)
                            case ApiKeys.TXN_OFFSET_COMMIT => handleTxnOffsetCommitRequest(request, requestLocal)
                            case ApiKeys.DESCRIBE_ACLS => handleDescribeAcls(request)
                            case ApiKeys.CREATE_ACLS => maybeForwardToController(request, handleCreateAcls)
                            case ApiKeys.DELETE_ACLS => maybeForwardToController(request, handleDeleteAcls)
                            case ApiKeys.ALTER_CONFIGS => handleAlterConfigsRequest(request)
                            case ApiKeys.DESCRIBE_CONFIGS => handleDescribeConfigsRequest(request)
                            case ApiKeys.ALTER_REPLICA_LOG_DIRS => handleAlterReplicaLogDirsRequest(request)
                            case ApiKeys.DESCRIBE_LOG_DIRS => handleDescribeLogDirsRequest(request)
                            case ApiKeys.SASL_AUTHENTICATE => handleSaslAuthenticateRequest(request)
                            case ApiKeys.CREATE_PARTITIONS => maybeForwardToController(request, handleCreatePartitionsRequest)
                            case ApiKeys.CREATE_DELEGATION_TOKEN => maybeForwardToController(request, handleCreateTokenRequest)
                            case ApiKeys.RENEW_DELEGATION_TOKEN => maybeForwardToController(request, handleRenewTokenRequest)
                            case ApiKeys.EXPIRE_DELEGATION_TOKEN => maybeForwardToController(request, handleExpireTokenRequest)
                            case ApiKeys.DESCRIBE_DELEGATION_TOKEN => handleDescribeTokensRequest(request)
                            case ApiKeys.DELETE_GROUPS => handleDeleteGroupsRequest(request, requestLocal)
                            case ApiKeys.ELECT_LEADERS => maybeForwardToController(request, handleElectLeaders)
                            case ApiKeys.INCREMENTAL_ALTER_CONFIGS => handleIncrementalAlterConfigsRequest(request)
                            case ApiKeys.ALTER_PARTITION_REASSIGNMENTS => maybeForwardToController(request, handleAlterPartitionReassignmentsRequest)
                            case ApiKeys.LIST_PARTITION_REASSIGNMENTS => maybeForwardToController(request, handleListPartitionReassignmentsRequest)
                            case ApiKeys.OFFSET_DELETE => handleOffsetDeleteRequest(request, requestLocal)
                            case ApiKeys.DESCRIBE_CLIENT_QUOTAS => handleDescribeClientQuotasRequest(request)
                            case ApiKeys.ALTER_CLIENT_QUOTAS => maybeForwardToController(request, handleAlterClientQuotasRequest)
                            case ApiKeys.DESCRIBE_USER_SCRAM_CREDENTIALS => handleDescribeUserScramCredentialsRequest(request)
                            case ApiKeys.ALTER_USER_SCRAM_CREDENTIALS => maybeForwardToController(request, handleAlterUserScramCredentialsRequest)
                            case ApiKeys.ALTER_PARTITION => handleAlterPartitionRequest(request)
                            case ApiKeys.UPDATE_FEATURES => maybeForwardToController(request, handleUpdateFeatures)
                            case ApiKeys.ENVELOPE => handleEnvelope(request, requestLocal)
                            case ApiKeys.DESCRIBE_CLUSTER => handleDescribeCluster(request)
                            case ApiKeys.DESCRIBE_PRODUCERS => handleDescribeProducersRequest(request)
                            case ApiKeys.UNREGISTER_BROKER => forwardToControllerOrFail(request)
                            case ApiKeys.DESCRIBE_TRANSACTIONS => handleDescribeTransactionsRequest(request)
                            case ApiKeys.LIST_TRANSACTIONS => handleListTransactionsRequest(request)
                            case ApiKeys.ALLOCATE_PRODUCER_IDS => handleAllocateProducerIdsRequest(request)
                            case ApiKeys.DESCRIBE_QUORUM => forwardToControllerOrFail(request)
                            case _ => throw new IllegalStateException(s"No handler for request api key ${request.header.apiKey}")
                        }
                    }
            }
        }
    }
}

```

## log管理

```scala

// log恢复
def loadLogs(defaultConfig: LogConfig, topicConfigOverrides: Map[String, LogConfig]): Unit = {
    for (dir <- liveLogDirs) {
        // 每个dir启动一个线程池
        val pool = Executors.newFixedThreadPool(numRecoveryThreadsPerDataDir,
        new LogRecoveryThreadFactory(logDirAbsolutePath))
        threadPools.append(pool)

        // 文件.kafka_cleanshutdown标记上次正常关闭
        val cleanShutdownFile = new File(dir, LogLoader.CleanShutdownFile)
        if (cleanShutdownFile.exists) {
            Files.deleteIfExists(cleanShutdownFile.toPath)
        }

        var recoveryPoints = Map[TopicPartition, Long]()
        // 读取文件recovery-point-offset-checkpoint
        // 文件行格式: topic partition offset
        recoveryPoints = this.recoveryPointCheckpoints(dir).read()

        var logStartOffsets = Map[TopicPartition, Long]()
        // 读取文件log-start-offset-checkpoint
        // 文件行格式: topic partition offset
        logStartOffsets = this.logStartOffsetCheckpoints(dir).read()

        // 文件夹命名格式: {topic}-{partition}[.xxx]
        val logsToLoad = Option(dir.listFiles).getOrElse(Array.empty).filter(logDir =>
            logDir.isDirectory && UnifiedLog.parseTopicPartitionName(logDir).topic != KafkaRaftServer.MetadataTopic)

        // 每个分区一个线程
        val jobsForDir = logsToLoad.map { logDir =>
            val runnable: Runnable = () => {
                // 恢复一个{topic}-{partition}的数据
                log = Some(loadLog(logDir, hadCleanShutdown, recoveryPoints, logStartOffsets,
                    defaultConfig, topicConfigOverrides, numRemainingSegments))
                {
                    // create the log directory if it doesn't exist
                    Files.createDirectories(dir.toPath)

                    // 读取文件leader-epoch-checkpoint
                    // 文件格式: epoch offset
                    val leaderEpochCache = UnifiedLog.maybeCreateLeaderEpochCache(dir, topicPartition, ...)
                    // 加载记录producer状态的{offset}.snapshot文件
                    val producerStateManager = new ProducerStateManager(topicPartition, dir, ...)

                    val offsets = new LogLoader(dir, topicPartition, ...).load()
                    {
                        val swapFiles = removeTempFilesAndCollectSwapFiles()
                        {
                            for (file <- dir.listFiles if file.isFile) {
                                if (filename.endsWith(DeletedFileSuffix) && !filename.endsWith(Snapshots.DELETE_SUFFIX)) {
                                    // 删除xxx.deleted，xxx非.checkpoint
                                    Files.deleteIfExists(file.toPath)
                                } else if (filename.endsWith(CleanedFileSuffix)) {
                                    // 查找{offset}.log.cleaned最小的offset
                                    minCleanedFileOffset = Math.min(offsetFromFile(file), minCleanedFileOffset)
                                    cleanedFiles += file
                                } else if (filename.endsWith(SwapFileSuffix)) {
                                    // 收集{offset}.log.swap文件
                                    swapFiles += file
                                }
                            }

                            // Delete all .swap files whose base offset is greater than the minimum .cleaned segment offset.
                            val (invalidSwapFiles, validSwapFiles) = swapFiles.partition(file => offsetFromFile(file) >= minCleanedFileOffset)
                            invalidSwapFiles.foreach { file =>
                                Files.deleteIfExists(file.toPath)
                            }

                            cleanedFiles.foreach { file =>
                                Files.deleteIfExists(file.toPath)
                            }

                            validSwapFiles
                        }

                        // 计算{offset}.log.swap文件offset范围
                        swapFiles.filter(f => UnifiedLog.isLogFile(new File(CoreUtils.replaceSuffix(f.getPath, SwapFileSuffix, ""))))
                        .foreach { f =>
                            val baseOffset = offsetFromFile(f)
                            // 读取每个swap文件
                            val segment = LogSegment.open(f.getParentFile, baseOffset = baseOffset, ..., fileSuffix = UnifiedLog.SwapFileSuffix)
                            minSwapFileOffset = Math.min(segment.baseOffset, minSwapFileOffset)
                            // 从索引查找最后一个offset
                            maxSwapFileOffset = Math.max(segment.readNextOffset, maxSwapFileOffset)
                        }

                        for (file <- dir.listFiles if file.isFile) {
                            if (!file.getName.endsWith(SwapFileSuffix)) {
                                val offset = offsetFromFile(file)
                                // 删除所有包含在swap文件中的非swap文件
                                if (offset >= minSwapFileOffset && offset < maxSwapFileOffset) {
                                    file.delete()
                                }
                            }
                        }

                        // Third pass: rename all swap files.
                        for (file <- dir.listFiles if file.isFile) {
                            if (file.getName.endsWith(SwapFileSuffix)) {
                                file.renameTo(new File(CoreUtils.replaceSuffix(file.getPath, UnifiedLog.SwapFileSuffix, "")))
                            }
                        }

                        // 读取所有segment文件
                        loadSegmentFiles()
                        {
                            for (file <- dir.listFiles.sortBy(_.getName) if file.isFile) {
                                // 删除没有对应log文件的索引文件
                                // {offset}.index/{offset}.txnindex/{offset}.timeindex
                                if (isIndexFile(file)) {
                                    val offset = offsetFromFile(file)
                                    val logFile = UnifiedLog.logFile(dir, offset)
                                    if (!logFile.exists) {
                                        Files.deleteIfExists(file.toPath)
                                    }
                                } else if (isLogFile(file)) {
                                    val baseOffset = offsetFromFile(file)
                                    val segment = LogSegment.open(dir = dir, baseOffset = baseOffset, ...)
                                    segments.add(segment)
                                }
                            }
                        }

                        val (newRecoveryPoint: Long, nextOffset: Long) = recoverLog()
                        {
                            
                        }

                        leaderEpochCache.foreach(_.truncateFromEnd(nextOffset))
                        val newLogStartOffset = math.max(logStartOffsetCheckpoint, segments.firstSegment.get.baseOffset)
                        leaderEpochCache.foreach(_.truncateFromStart(logStartOffsetCheckpoint))

                        // 加载{offset}.snapshot文件
                        producerStateManager.removeStraySnapshots(segments.baseOffsets.toSeq)
                    }

                    val localLog = new LocalLog(dir, config, segments, 
                                    offsets.recoveryPoint, offsets.nextOffsetMetadata, ...)

                    new UnifiedLog(offsets.logStartOffset, localLog, ...)
                }
            }
            runnable
        }

        jobs += jobsForDir.map(pool.submit)
    }

    // 等待log恢复完毕
    for (dirJobs <- jobs) {
        dirJobs.foreach(_.get)
    }
    threadPools.foreach(_.shutdown())
}

// log相关定时任务
scheduler.schedule("kafka-log-retention", cleanupLogs _, ...)
scheduler.schedule("kafka-log-flusher", flushDirtyLogs _, ...)

scheduler.schedule("kafka-recovery-point-checkpoint", checkpointLogRecoveryOffsets _, ...)
{
    val recoveryOffsets = logsToCheckpoint.map { case (tp, log) => tp -> log.recoveryPoint }
    checkpoint.write(recoveryOffsets)
    {
        checkpointFile.write(entries.toSeq.asJava)
        {
            try (FileOutputStream fileOutputStream = new FileOutputStream(tempPath.toFile());
                 BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(fileOutputStream, UTF_8))) {
                // Write the version
                writer.write(Integer.toString(version));
                writer.newLine();

                // Write the entries count
                writer.write(Integer.toString(entries.size()));
                writer.newLine();

                // Write each entry on a new line.
                for (T entry : entries) {
                    writer.write(formatter.toString(entry));
                    writer.newLine();
                }

                writer.flush();
                fileOutputStream.getFD().sync();
            }

            Utils.atomicMoveWithFallback(tempPath, absolutePath);
        }
    }
}

scheduler.schedule("kafka-log-start-offset-checkpoint", checkpointLogStartOffsets _, ...)
{
    val logStartOffsets = logsToCheckpoint.collect {
        case (tp, log) if log.logStartOffset > log.logSegments.head.baseOffset => tp -> log.logStartOffset
    }
    checkpoint.write(logStartOffsets)
}

scheduler.schedule("kafka-delete-logs", deleteLogs _, ...)

if (cleanerConfig.enableCleaner) {
    _cleaner = new LogCleaner(cleanerConfig, liveLogDirs, currentLogs, logDirFailureChannel, time = time)
    _cleaner.startup()
}

```

## 生产

### client

```scala
object ConsoleProducer {

    def main(args: Array[String]): Unit = {
        val config = new ProducerConfig(args)
        val props = producerProps(config)
        {
            props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServer)
            // producer标识
            props.put(ProducerConfig.CLIENT_ID_CONFIG, "console-producer")
            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.ByteArraySerializer")
            props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.ByteArraySerializer")
        
            // 异步发送等待最大时长
            props.put(ProducerConfig.LINGER_MS_CONFIG, config.sendTimeoutOpt)
            // ack: -1、0、1，默认-1
            props.put(ProducerConfig.ACKS_CONFIG, config.requestRequiredAcksOpt)
            // ack timeout
            props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, config.requestTimeoutMsOpt)
            // 重试次数
            props.put(ProducerConfig.RETRIES_CONFIG, config.messageSendMaxRetriesOpt)
            // 重试间隔
            props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, config.retryBackoffMsOpt)
            // tcp send buf
            props.put(ProducerConfig.SEND_BUFFER_CONFIG, config.socketBufferSizeOpt)
            // 发送给server之前暂存message的内存
            props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, config.maxMemoryBytesOpt)
            // batch消息条数
            props.put(ProducerConfig.BATCH_SIZE_CONFIG, config.batchSizeOpt)
            // batch消息大小/每个分区
            props.put(ProducerConfig.BATCH_SIZE_CONFIG, config.maxPartitionMemoryBytesOpt)
            // metadata刷新毫秒数
            props.put(ProducerConfig.METADATA_MAX_AGE_CONFIG, config.metadataExpiryMsOpt)
            // 发送请求最大阻塞时长
            props.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, config.maxBlockMsOpt)
        }

        val producer = new KafkaProducer[Array[Byte], Array[Byte]](props)

        send(producer, record, config.sync)
        {
            if (sync)
                producer.send(record).get()
                {
                    producer.send(record, null)
                }
            else
                producer.send(record, new ErrorLoggingCallback(...))
        }
    }
}
```

```java
public class KafkaProducer<K, V> implements Producer<K, V> {

    private ProducerConfig producerConfig;
    private Partitioner partitioner;

    KafkaProducer(ProducerConfig config, ...) {
        String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG);

        // 消息分区
        this.partitioner = config.getConfiguredInstance(
                    ProducerConfig.PARTITIONER_CLASS_CONFIG,
                    Partitioner.class,
                    Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
        {
            Object o = Utils.newInstance(config.get(ProducerConfig.PARTITIONER_CLASS_CONFIG), Partitioner.class);
            if (o instanceof Configurable)
                ((Configurable) o).configure(configPairs);
        }

        // 消息拦截器： onSend(...) & onAcknowledgement(...)
        List<ProducerInterceptor<K, V>> interceptorList = (List) config.getConfiguredInstances(
                    ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,
                    ProducerInterceptor.class,
                    Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
        this.interceptors = new ProducerInterceptors<>(interceptorList);

        // 在内存中累加消息
        this.accumulator = new RecordAccumulator(
                    batchSize,
                    lingerMs(config),
                    retryBackoffMs,
                    deliveryTimeoutMs,
                    partitionerConfig,
                    new BufferPool(this.totalMemorySize, batchSize, ...));
        
        List<InetSocketAddress> addresses = config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG);

        // 元数据缓存
        this.metadata = new ProducerMetadata(retryBackoffMs,
                        config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),
                        config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG), ...);
        this.metadata.bootstrap(addresses);

        this.sender = newSender(logContext, kafkaClient, this.metadata);
        {
            int maxInflightRequests = producerConfig.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION);
            int requestTimeoutMs = producerConfig.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);

            KafkaClient client = new NetworkClient(new Selector(...), 
                metadata,
                clientId,
                maxInflightRequests,
                producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),
                producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),
                producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
                producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
                requestTimeoutMs,
                producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),
                producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG), ...);
        }

        this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
        this.ioThread.start();
    }

    // 发送消息
    public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {
        // 发送之前先经过拦截器处理
        ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);
        return doSend(interceptedRecord, callback);
        {
            byte[] serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
            byte[] serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
            
            // 计算分区
            int partition = partition(record, serializedKey, serializedValue, cluster);
            {
                // 消息指定分区
                if (record.partition() != null)
                    return record.partition();

                // 自定义分区
                if (partitioner != null) {
                    return partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);
                }

                // 默认按hash(key)分区
                if (serializedKey != null) {
                    return BuiltInPartitioner.partitionForKey(serializedKey, cluster.partitionsForTopic(record.topic()).size());
                }
            }

            int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(compressionType, serializedKey, serializedValue, headers);
            if (size > maxRequestSize || size > totalMemorySize)
                throw new RecordTooLargeException(...);
            
            long timestamp = record.timestamp() == null ? nowMs : record.timestamp();

            RecordAccumulator.RecordAppendResult result = accumulator.append(record.topic(), 
                    partition, timestamp, serializedKey, serializedValue, headers, ...);
            {
                TopicInfo topicInfo = topicInfoMap.computeIfAbsent(topic, k -> new TopicInfo(logContext, k, batchSize));
                Deque<ProducerBatch> dq = topicInfo.batches.computeIfAbsent(effectivePartition, k -> new ArrayDeque<>());
                RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callbacks, dq, nowMs);
                {
                    ProducerBatch last = deque.peekLast();
                    return last.tryAppend(timestamp, key, value, headers, callback, nowMs);
                    {
                        // 当前批次内存不足
                        if (!recordsBuilder.hasRoomFor(timestamp, key, value, headers)) {
                            return null;
                        }

                        this.recordsBuilder.append(timestamp, key, value, headers);
                        {
                            // offset每次加1
                            appendWithOffset(offset, false, timestamp, key, value, headers);
                            {
                                appendDefaultRecord(offset, timestamp, key, value, headers);
                                {
                                    // 与批次第一条记录的差值: offset表示记录条数
                                    int offsetDelta = (int) (offset - baseOffset);
                                    long timestampDelta = timestamp - baseTimestamp;

                                    // 记录格式
                                    int sizeInBytes = DefaultRecord.writeTo(appendStream, offsetDelta, timestampDelta, key, value, headers);
                                    {
                                        int sizeInBytes = sizeOfBodyInBytes(offsetDelta, timestampDelta, key, value, headers);
                                        ByteUtils.writeVarint(sizeInBytes, out);

                                        byte attributes = 0; // there are no used record attributes at the moment
                                        out.write(attributes);

                                        ByteUtils.writeVarlong(timestampDelta, out);
                                        ByteUtils.writeVarint(offsetDelta, out);

                                        if (key == null) {
                                            ByteUtils.writeVarint(-1, out);
                                        } else {
                                            int keySize = key.remaining();
                                            ByteUtils.writeVarint(keySize, out);
                                            Utils.writeTo(out, key, keySize);
                                        }

                                        if (value == null) {
                                            ByteUtils.writeVarint(-1, out);
                                        } else {
                                            int valueSize = value.remaining();
                                            ByteUtils.writeVarint(valueSize, out);
                                            Utils.writeTo(out, value, valueSize);
                                        }

                                        if (headers == null)
                                            throw new IllegalArgumentException("Headers cannot be null");

                                        ByteUtils.writeVarint(headers.length, out);

                                        for (Header header : headers) {
                                            String headerKey = header.key();
                                            if (headerKey == null)
                                                throw new IllegalArgumentException("Invalid null header key found in headers");

                                            byte[] utf8Bytes = Utils.utf8(headerKey);
                                            ByteUtils.writeVarint(utf8Bytes.length, out);
                                            out.write(utf8Bytes);

                                            byte[] headerValue = header.value();
                                            if (headerValue == null) {
                                                ByteUtils.writeVarint(-1, out);
                                            } else {
                                                ByteUtils.writeVarint(headerValue.length, out);
                                                out.write(headerValue);
                                            }
                                        }

                                        return ByteUtils.sizeOfVarint(sizeInBytes) + sizeInBytes;
                                    }
                                    
                                    recordWritten(offset, timestamp, sizeInBytes);
                                    {
                                        numRecords += 1;
                                        uncompressedRecordsSizeInBytes += size;
                                        lastOffset = offset;
                                    }
                                }
                            }
                        }

                        return new FutureRecordMetadata(this.produceFuture, this.recordCount, timestamp, ...);
                    }   
                }

                if (appendResult != null) {
                    return appendResult;
                }

                // 当前批次已满，开启新的批次
                buffer = free.allocate(size, maxTimeToBlock);
                return appendNewBatch(topic, effectivePartition, dq, timestamp, key, value, headers, callbacks, buffer, nowMs);
                {
                    MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, apiVersions.maxUsableProduceMagic());
                    ProducerBatch batch = new ProducerBatch(new TopicPartition(topic, partition), recordsBuilder, nowMs);
                    dq.addLast(batch);
                }
            }

            // 之前的批次已满，创建新的批次，再次尝试
            if (result.abortForNewBatch) {
                result = accumulator.append(record.topic(), partition, timestamp, serializedKey, serializedValue, headers, ...);
            }

            // 开始发送
            if (result.batchIsFull || result.newBatchCreated) {
                this.sender.wakeup();
            }

            return result.future;
        }
    }

}

// 发送线程
public class Sender implements Runnable {

    @Override
    public void run() {
        while (running) {
            runOnce();
            {
                long currentTimeMs = time.milliseconds();
                long pollTimeout = sendProducerData(currentTimeMs);
                {
                    // 更新元数据
                    Cluster cluster = metadata.fetch();
                    // get the list of partitions with data ready to send
                    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);
                    if (!result.unknownLeaderTopics.isEmpty()) {
                        for (String topic : result.unknownLeaderTopics)
                            this.metadata.add(topic, now);
                        
                        this.metadata.requestUpdate();
                    }

                    Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);
                    
                    // 加入inflight列表
                    addToInflightBatches(batches);
                    {
                        for (List<ProducerBatch> batchList : batches.values()) {
                            for (ProducerBatch batch : batchList) {
                                List<ProducerBatch> inflightBatchList = inFlightBatches.get(batch.topicPartition);
                                inflightBatchList.add(batch);
                            }
                        }
                    }

                    // 保证消息顺序: 每个partition只能有一个inflight的消息
                    if (guaranteeMessageOrder) {
                        for (List<ProducerBatch> batchList : batches.values()) {
                            for (ProducerBatch batch : batchList)
                                this.accumulator.mutePartition(batch.topicPartition);
                        }
                    }

                    // 检查inflight的消息是否过期: ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG
                    List<ProducerBatch> expiredInflightBatches = getExpiredInflightBatches(now);
                    // 检查未发送的消息是否过期: ProducerConfig.LINGER_MS_CONFIG
                    List<ProducerBatch> expiredBatches = this.accumulator.expiredBatches(now);
                    expiredBatches.addAll(expiredInflightBatches);

                    // 只是打印TimeoutException
                    for (ProducerBatch expiredBatch : expiredBatches) {
                        if (expiredBatch.completeExceptionally(topLevelException, recordExceptions)) {
                            maybeRemoveAndDeallocateBatch(expiredBatch);
                        }
                    }

                    // 发送消息
                    sendProduceRequests(batches, now);
                    {
                        sendProduceRequest(now, entry.getKey(), acks, requestTimeoutMs, entry.getValue());
                        {
                            Map<TopicPartition, ProducerBatch> recordsByPartition = new HashMap<>(batches.size());
                            for (ProducerBatch batch : batches) {
                                TopicPartition tp = batch.topicPartition;
                                recordsByPartition.put(tp, batch);
                            }

                            ProduceRequest.Builder requestBuilder = ProduceRequest.forMagic(minUsedMagic,
                                new ProduceRequestData()
                                        .setAcks(acks)
                                        .setTimeoutMs(timeout)
                                        .setTransactionalId(transactionalId)
                                        .setTopicData(tpd));

                            ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, requestTimeoutMs, callback);
                            client.send(clientRequest, now);
                            {
                                doSend(request, false, now);
                                {
                                    // 检查inflight请求个数
                                    if (!canSendRequest(nodeId, now))
                                        throw new IllegalStateException(...);
                                    
                                    String destination = clientRequest.destination();
                                    RequestHeader header = clientRequest.makeHeader(request.version());
                                    Send send = request.toSend(header);
                                    InFlightRequest inFlightRequest = new InFlightRequest(
                                            clientRequest,
                                            header,
                                            isInternalRequest,
                                            request,
                                            send,
                                            now);
                                    this.inFlightRequests.add(inFlightRequest);
                                    selector.send(new NetworkSend(clientRequest.destination(), send));
                                }
                            }
                        }
                    }

                    // 下一个批次过期时间
                    return pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);
                }

                client.poll(pollTimeout, currentTimeMs);
                {
                    // 接收响应信息
                    this.selector.poll(Utils.min(timeout, metadataTimeout, defaultRequestTimeoutMs));

                    long updatedNow = this.time.milliseconds();
                    List<ClientResponse> responses = new ArrayList<>();

                    // 处理ack，inflight转完成
                    handleCompletedSends(responses, updatedNow);
                    handleCompletedReceives(responses, updatedNow);
                    handleDisconnections(responses, updatedNow);
                    handleConnections();
                    handleInitiateApiVersionRequests(updatedNow);
                    handleTimedOutConnections(responses, updatedNow);
                    handleTimedOutRequests(responses, updatedNow);
                    completeResponses(responses);
                }
            }
        }

        while (this.accumulator.hasUndrained() || this.client.inFlightRequestCount() > 0) {
            runOnce();
        }
    }
}
```

### server

```scala

class KafkaApis(...) {

    override def handle(request: RequestChannel.Request, requestLocal: RequestLocal): Unit = {

        handleProduceRequest(request, requestLocal)
        {
            val produceRequest = request.body[ProduceRequest]

            replicaManager.appendRecords(
                timeout = produceRequest.timeout.toLong,
                requiredAcks = produceRequest.acks,
                internalTopicsAllowed = internalTopicsAllowed,
                origin = AppendOrigin.Client,
                entriesPerPartition = authorizedRequestInfo,
                requestLocal = requestLocal,
                responseCallback = sendResponseCallback,
                recordConversionStatsCallback = processingStatsCallback)
            {
                val localProduceResults = appendToLocalLog(..., requiredAcks, requestLocal)
                {
                    entriesPerPartition.map { case (topicPartition, records) =>
                        val partition = getPartitionOrException(topicPartition)
                        partition.appendRecordsToLeader(records, origin, requiredAcks, requestLocal)
                        {
                            // 写入leader分区
                            leaderLog.appendAsLeader(records, leaderEpoch = this.leaderEpoch, ...)
                            {
                                // 产生新的segment
                                maybeRoll(validRecords.sizeInBytes, appendInfo)

                                // 幂等producer保存最近5个批次的元数据用于判断重复
                                val maybeDuplicate = analyzeAndValidateProducerState(logOffsetMetadata, validRecords, origin)
                                {
                                    // 幂等生产者必须配置producerId
                                    if (batch.hasProducerId) {
                                        // 重复条件: (producerId, producerEpoch, firstSeq, lastSeq)
                                        maybeLastEntry.flatMap(_.findDuplicateBatch(batch)).foreach { duplicate =>
                                            return (updatedProducers, completedTxns.toList, Some(duplicate))
                                        }
                                    }
                                }

                                localLog.append(appendInfo.lastOffset, appendInfo.maxTimestamp, appendInfo.offsetOfMaxTimestamp, validRecords)
                                {
                                    segments.activeSegment.append(...);
                                    {
                                        val physicalPosition = log.sizeInBytes()

                                        // append the messages
                                        log.append(records)

                                        // 每生产indexIntervalBytes这么多消息就插入一条索引记录
                                        // append an entry to the index (if needed)
                                        if (bytesSinceLastIndexEntry > indexIntervalBytes) {

                                            // 最大的offset所在的batch在文件中的位置
                                            offsetIndex.append(largestOffset, physicalPosition)
                                            {
                                                // 相对于segment的第一个log的偏移量
                                                val relativeOffset = largestOffset - baseOffset
                                                mmap.putInt(relativeOffset)
                                                mmap.putInt(position)
                                            }

                                            // 时间戳递增，记录时间戳对应的记录的offset
                                            timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestampSoFar)
                                            {
                                                if (maxTimestampSoFar > lastEntry.timestamp) {
                                                    mmap.putLong(maxTimestampSoFar)
                                                    mmap.putInt(relativeOffset(offsetOfMaxTimestampSoFar))
                                                }
                                            }

                                            bytesSinceLastIndexEntry = 0
                                        }
                                        bytesSinceLastIndexEntry += records.sizeInBytes
                                    }
                                }

                                updateHighWatermarkWithLogEndOffset()
                            }

                            // HW=high-watermark，表示所有replica都取走的最大消息
                            // 如果ISR=1，直接更新HW
                            maybeIncrementLeaderHW(leaderLog)
                        }
                    }
                }
                
                // HW变化时检查ack=all的生产请求是否满足
                actionQueue.add {
                  () =>
                    localProduceResults.foreach {
                        case (topicPartition, result) =>
                        val requestKey = TopicPartitionOperationKey(topicPartition)
                        // #replica#3.3 HighWatermark发生变化，判断延迟操作是否满足
                        result.info.leaderHwChange match {
                            case LeaderHwChange.Increased =>
                            // some delayed operations may be unblocked after HW changed
                            delayedProducePurgatory.checkAndComplete(requestKey)
                            delayedFetchPurgatory.checkAndComplete(requestKey)
                            delayedDeleteRecordsPurgatory.checkAndComplete(requestKey)
                        }
                    }
                }

                // ack=-1延迟响应
                if (delayedProduceRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) {
                    // create delayed produce operation
                    val produceMetadata = ProduceMetadata(requiredAcks, produceStatus)
                    // 延迟生产
                    val delayedProduce = new DelayedProduce(timeout, produceMetadata, this, responseCallback, delayedProduceLock)
                    // key = {topic}-{partition}
                    val producerRequestKeys = entriesPerPartition.keys.map(TopicPartitionOperationKey(_)).toSeq
                    delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)

                } else {
                    // we can respond immediately
                    val produceResponseStatus = produceStatus.map { case (k, status) => k -> status.responseStatus }
                    responseCallback(produceResponseStatus)
                }
            }
        }

    }

}

```

## 消费

### client

```scala

object ConsoleConsumer {

    def main(args: Array[String]): Unit = {
        val conf = new ConsumerConfig(args)

        run(conf)
        {
            val timeoutMs = conf.timeoutMs.toLong
            val consumer = new KafkaConsumer(consumerProps(conf), new ByteArrayDeserializer, new ByteArrayDeserializer)
            {
                List<ConsumerInterceptor<K, V>> interceptorList = (List) config.getConfiguredInstances(
                    ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG,
                    ConsumerInterceptor.class,
                    Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId));
                this.interceptors = new ConsumerInterceptors<>(interceptorList);

                this.keyDeserializer = config.getConfiguredInstance(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, Deserializer.class);
                this.keyDeserializer.configure(config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId)), true);

                this.valueDeserializer = config.getConfiguredInstance(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, Deserializer.class);
                this.valueDeserializer.configure(config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId)), false);

                OffsetResetStrategy offsetResetStrategy = OffsetResetStrategy.valueOf(config.getString(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));
                this.subscriptions = new SubscriptionState(..., offsetResetStrategy);

                List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(...);

                ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config, time, logContext);
                NetworkClient netClient = new NetworkClient(new Selector(...), ...);
                this.client = new ConsumerNetworkClient(netClient, ...);

                this.coordinator = new ConsumerCoordinator(groupRebalanceConfig, this.client, ...);
                this.fetcher = new Fetcher<>(this.client, ...);
            }

            val consumerWrapper = new ConsumerWrapper(Option(conf.topicArg), None, None, Option(conf.includedTopicsArg), consumer, timeoutMs)
            process(conf.maxMessages, ..., consumerWrapper, ...)
            {
                consumerWrapper.receive();
                {
                    while (!recordIter.hasNext) {
                        // 取下一批
                        recordIter = consumer.poll(Duration.ofMillis(timeoutMs)).iterator
                        {
                            Fetch<K, V> fetch = pollForFetches(timer);
                            {
                                fetcher.sendFetches();
                                {
                                    final FetchRequest.Builder request = FetchRequest.Builder
                                        .forConsumer(maxVersion, this.maxWaitMs, this.minBytes, data.toSend())
                                        .isolationLevel(isolationLevel)
                                        .setMaxBytes(this.maxBytes)
                                        .metadata(data.metadata())
                                        .removed(data.toForget())
                                        .replaced(data.toReplace())
                                        .rackId(clientRackId);

                                    RequestFuture<ClientResponse> future = client.send(fetchTarget, request);
                                    future.addListener(...);
                                }

                                return fetcher.collectFetch();
                            }

                            return this.interceptors.onConsume(new ConsumerRecords<>(fetch.records()));
                        }
                    }

                    recordIter.next
                }
            }
        }
    }

}

```

### server

```scala

class KafkaApis(...) {

    override def handle(request: RequestChannel.Request, requestLocal: RequestLocal): Unit = {
        handleFetchRequest(request)
        {
            val versionId = request.header.apiVersion
            val clientId = request.header.clientId
            val fetchRequest = request.body[FetchRequest]

            val params = FetchParams(
                requestVersion = versionId,
                replicaId = fetchRequest.replicaId,
                maxWaitMs = fetchRequest.maxWait,
                minBytes = fetchMinBytes,
                maxBytes = fetchMaxBytes,
                // 隔离等级：FetchLogEnd/FetchTxnCommitted/FetchHighWatermark
                isolation = FetchIsolation(fetchRequest),
                clientMetadata = clientMetadata
            )

            // call the replica manager to fetch messages from the local replica
            replicaManager.fetchMessages(
                params = params,
                fetchInfos = interesting,
                quota = replicationQuota(fetchRequest),
                responseCallback = processResponseCallback)
            {
                val logReadResults = readFromLocalLog(params, fetchInfos, quota, readFromPurgatory = false)
                {
                    readPartitionInfo.foreach { case (tp, fetchInfo) =>
                        read(tp, fetchInfo, limitBytes, minOneMessage)
                        {
                            val offset = fetchInfo.fetchOffset
                            val partitionFetchSize = fetchInfo.maxBytes
                            val followerLogStartOffset = fetchInfo.logStartOffset

                            val preferredReadReplica = params.clientMetadata.flatMap(
                                metadata => findPreferredReadReplica(partition, metadata, params.replicaId, fetchInfo.fetchOffset, fetchTimeMs))
                                {
                                    // 副本选择策略
                                    replicaSelector.select(partition.topicPartition, clientMetadata, partitionInfo)
                                }

                            if (preferredReadReplica.isDefined) {
                                partition.fetchOffsetSnapshot(fetchInfo.currentLeaderEpoch, fetchOnlyFromLeader = false)
                            } else {
                                partition.fetchRecords(...)
                                {
                                    if (fetchParams.isFromFollower) {
                                        readFromLocalLog(localLog)
                                    } else {
                                        readFromLocalLog(localLog)
                                        {
                                            localLog.read(fetchOffset, maxBytes, ...)
                                            {
                                                val maxOffsetMetadata = isolation match {
                                                    case FetchLogEnd => localLog.logEndOffsetMetadata
                                                    case FetchHighWatermark => fetchHighWatermarkMetadata
                                                    case FetchTxnCommitted => fetchLastStableOffsetMetadata
                                                }

                                                localLog.read(startOffset, maxLength, minOneMessage, maxOffsetMetadata, ...)
                                                {
                                                    // 先定位到segment
                                                    var segmentOpt = segments.floorSegment(startOffset)
                                                    val segment = segmentOpt.get
                                                    val baseOffset = segment.baseOffset

                                                    segment.read(startOffset, maxLength, maxPosition, minOneMessage)
                                                    {
                                                        // 查找索引中的文件位置
                                                        val startOffsetAndSize = translateOffset(startOffset)
                                                        val startPosition = startOffsetAndSize.position
                                                        val offsetMetadata = LogOffsetMetadata(startOffset, this.baseOffset, startPosition)
                                                        FetchDataInfo(offsetMetadata, log.slice(startPosition, fetchSize), ...)
                                                        {
                                                            // FileRecords: 只记录fd和位置信息，实际发送数据时zero-copy
                                                            return new FileRecords(file, channel, startPosition, startPosition + availableBytes, true);
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                            
                        }
                    }
                }

                // respond immediately if 1) fetch request does not want to wait
                //                        2) fetch request does not require any data
                //                        3) has enough data to respond
                //                        4) some error happens while reading data
                //                        5) we found a diverging epoch
                //                        6) has a preferred read replica
                if (params.maxWaitMs <= 0 || fetchInfos.isEmpty || bytesReadable >= params.minBytes 
                    || errorReadingData || hasDivergingEpoch || hasPreferredReadReplica) {
                    responseCallback(fetchPartitionData)
                } else {
                    // 等到累积一定量的数据或超时再响应
                    val delayedFetch = new DelayedFetch(
                        params = params,
                        fetchPartitionStatus = fetchPartitionStatus,
                        replicaManager = this,
                        quota = quota,
                        responseCallback = responseCallback
                    )

                    delayedFetchPurgatory.tryCompleteElseWatch(delayedFetch, delayedFetchKeys)
                }
            }
        }
    }

}

```