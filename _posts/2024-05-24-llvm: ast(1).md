---
layout: post
title:  "ast(1)"
date:   2024-05-24 21:49:07 +0000
categories: llvm
tags: llvm
---

# ast

书接上文，继续分析从`c11`源码转换为`AST`的过程，主流程如下：

```c++

/// Abstract base class to use for AST consumer-based frontend actions.
class ASTFrontendAction : public FrontendAction {
protected:
    /// Implement the ExecuteAction interface by running Sema 
    /// on the already-initialized AST consumer.
    void ExecuteAction() override;
    {
        /// Sema - implements semantic analysis and AST building for C.
        CI.createSema(getTranslationUnitKind(), ...);
        {
            TheSema.reset(new Sema(getPreprocessor(), getASTContext(), getASTConsumer(), ...));
        }
        // 预处理、词法分析、语法分析、AST生成
        ParseAST(CI.getSema(), ...);
    }
}

// 主流程
void clang::ParseAST(Sema &S, ...) {
    // ASTConsumer - 对生成的AST进一步处理
    ASTConsumer *Consumer = &S.getASTConsumer();

    // Preprocessor - 预处理源文件
    Preprocessor PP = S.getPreprocessor();

    // Parser - 解析源文件生成AST
    std::unique_ptr<Parser> ParseOP(new Parser(PP, S, ...));
    Parser &P = *ParseOP.get();

    // 开始处理源文件
    S.getPreprocessor().EnterMainSourceFile();

    // 获取第一个token
    P.Initialize();

    // 根据c11的语法规则，源文件是由一组声明语句组成的
    //
    // translation-unit:
    //     external-declaration
    //     translation-unit external-declaration
    //
    // external-declaration:
    //     function-definition
    //     declaration
    //
    // 每次声明可能包含多个变量，例如：`int a,b;`，所以类型是`DeclGroup`
    Parser::DeclGroupPtrTy ADecl;
    for (bool AtEOF = P.ParseFirstTopLevelDecl(ADecl, ImportState); 
         !AtEOF;
         AtEOF = P.ParseTopLevelDecl(ADecl, ImportState)) {
        // Consumer来决定如何使用解析的结果
        Consumer->HandleTopLevelDecl(ADecl.get());
    }

    Consumer->HandleTranslationUnit(S.getASTContext());
}

```

## 核心类

搞明白整个流程的关键是梳理几个核心类的关联关系及各自的职责

```c++

/// Parser - This implements a parser for the C family of languages.
/// Parser主要职责是根据语法规则解析语法单元，同时驱动Sema完成AST构造过程
class Parser : public CodeCompletionHandler {
private:
    // 预处理器
    Preprocessor &PP;

    /// 当前有效的token
    /// Tok - The current token we are peeking ahead.  
    /// All parsing methods assume that this is valid.
    Token Tok;

    /// Actions - These are the callbacks we invoke as we parse various constructs in the file.
    Sema &Actions;

public:
    Preprocessor &getPreprocessor() const { return PP; }
    Sema &getActions() const { return Actions; }
    const Token &getCurToken() const { return Tok; }

    /// Initialize - Warm up the parser.
    void Initialize();

    /// Parse the first top-level declaration in a translation unit.
    bool ParseFirstTopLevelDecl(DeclGroupPtrTy &Result, ...);

    /// ParseTopLevelDecl - Parse one top-level declaration.
    bool ParseTopLevelDecl(DeclGroupPtrTy &Result, ...);

    ExprResult ParseExpression(...);
    ExprResult ParseCaseExpression(...);
    ExprResult ParseAssignmentExpression(...);
    ExprResult ParseUnaryExprOrTypeTraitExpression();
    ExprResult ParseRHSOfBinaryExpression(ExprResult LHS, ...);
    ExprResult ParseParenExpression(...);

    StmtResult ParseStatement(...);
    StmtResult ParseExprStatement(...);
    StmtResult ParseLabeledStatement(...);
    StmtResult ParseCaseStatement(...);
    StmtResult ParseDefaultStatement(...);
    StmtResult ParseCompoundStatement(...);
    StmtResult ParseIfStatement(...);
    StmtResult ParseSwitchStatement(...);
    StmtResult ParseWhileStatement(...);
    StmtResult ParseDoStatement();
    StmtResult ParseForStatement(...);
    StmtResult ParseGotoStatement();
    StmtResult ParseContinueStatement();
    StmtResult ParseBreakStatement();
    StmtResult ParseReturnStatement();
    StmtResult ParseAsmStatement(...);
}

/// 管理lexer栈产生token，处理各种宏指令
class Preprocessor {
    // 语言的各种特性开关
    const LangOptions &LangOpts;

    // cpu/os等平台信息
    const TargetInfo *Target = nullptr;
    const TargetInfo *AuxTarget = nullptr;

    // 源文件管理
    FileManager       &FileMgr;
    SourceManager     &SourceMgr;
    HeaderSearch      &HeaderInfo;

    /// 核心对象CurLexer和CurTokenLexer分别表示解析源文件和宏扩展

    /// The current top of the stack that we're lexing from if
    /// not expanding a macro and we are lexing directly from source code.
    /// Only one of CurLexer, or CurTokenLexer will be non-null.
    std::unique_ptr<Lexer> CurLexer;

    /// The current macro we are expanding, if we are expanding a macro.
    /// One of CurLexer and CurTokenLexer must be null.
    std::unique_ptr<TokenLexer> CurTokenLexer;

    // 通过callback选择CurLexer或CurTokenLexer
    /// The kind of lexer we're currently working with.
    typedef bool (*LexerCallback)(Preprocessor &, Token &);
    LexerCallback CurLexerCallback = &CLK_Lexer;

    /// 此结构用于处理`#include`，类似函数调用产生的栈帧
    /// Keeps track of the stack of files currently `#included`,
    /// and macros currently being expanded from, not counting CurLexer/CurTokenLexer.
    struct IncludeStackInfo {
        LexerCallback               CurLexerCallback;
        std::unique_ptr<Lexer>      TheLexer;
        PreprocessorLexer          *ThePPLexer;
        std::unique_ptr<TokenLexer> TheTokenLexer;
    }
    std::vector<IncludeStackInfo> IncludeMacroStack;

    /// map结构的宏定义符号表，记录每个标识符定义的macro的历史
    /// For each IdentifierInfo that was associated with a macro, we
    /// keep a mapping to the history of all macro definitions and #undefs in
    /// the reverse order (the latest one is in the head of the list).
    ///
    /// This mapping lives within the `CurSubmoduleState`.
    using MacroMap = llvm::DenseMap<const IdentifierInfo *, MacroState>;

    /// Information about a submodule's preprocessor state.
    struct SubmoduleState {
        /// The macros for the submodule.
        MacroMap Macros;
    };
    SubmoduleState *CurSubmoduleState;

    /// 包含系统内置的变量以及命令行通过-Dkey=val指定的变量
    /// The predefined macros that preprocessor should use from the command line etc.
    std::string Predefines;

public:
    /// Enter the specified FileID as the main source file,
    /// which implicitly adds the builtin defines etc.
    void EnterMainSourceFile();
    {
        // 加载当前文件到内存
        EnterSourceFile(MainFileID, ...);

        // 加载系统预定义的变量
        std::unique_ptr<llvm::MemoryBuffer> SB = llvm::MemoryBuffer::getMemBufferCopy(Predefines, "<built-in>");
        FileID FID = SourceMgr.createFileID(std::move(SB));
        // 此处的效果就是源文件的开头处定义了这些系统变量！！！
        EnterSourceFile(FID, ...);
    }

    /// Add a source file to the top of the include stack and
    /// start lexing tokens from it instead of the current buffer.
    bool EnterSourceFile(FileID FID, ...);
    {
            // 构造词法解析器
            Lexer *TheLexer = new Lexer(FID, *InputFile, *this, ...);
            EnterSourceFileWithLexer(TheLexer, CurDir);
            {
                // 每个源文件都有配套的多个lexer以及关联信息，这些信息的组织结构类似函数的栈帧
                // a调用b，b调用c，会给a/b/c都创建栈帧结构存储各自的信息
                // 同样，a通过`#include`包含b，b通过`#include`包含c，a/b/c各自都会有自己的lexer，然后以类似栈帧的结构组织
                if (CurPPLexer || CurTokenLexer)
                    // 保存lexer
                    PushIncludeMacroStack();
                    {
                        IncludeMacroStack.emplace_back(..., std::move(CurLexer), CurPPLexer, std::move(CurTokenLexer), CurDirLookup);
                    }
                CurLexer.reset(TheLexer);
                CurPPLexer = TheLexer;
                CurDirLookup = CurDir;
                // callback指向Lexer
                CurLexerCallback = CLK_Lexer;
            }
        }

    /// Add a Macro to the top of the include stack and start lexing
    /// tokens from it instead of the current buffer.
    void EnterMacro(Token &Tok, ..., MacroInfo *Macro, MacroArgs *Args);
    {
        std::unique_ptr<TokenLexer> TokLexer = std::make_unique<TokenLexer>(Tok, ILEnd, Macro, Args, *this);
        
        // 保存当前的lexer
        PushIncludeMacroStack();
        CurDirLookup = nullptr;
        // 替换为macro扩展lexer
        CurTokenLexer = std::move(TokLexer);
        // callback指向TokenLexer
        CurLexerCallback = CLK_TokenLexer;
    }

    /// Callback invoked when the lexer hits the end of the current file.
    bool HandleEndOfFile(Token &Result, bool isEndOfMacro = false);
    {
        if (!IncludeMacroStack.empty()) {
            // 还原lexer
            RemoveTopOfLexerStack();
            {
                PopIncludeMacroStack();
                {
                    CurLexer = std::move(IncludeMacroStack.back().TheLexer);
                    CurPPLexer = IncludeMacroStack.back().ThePPLexer;
                    CurTokenLexer = std::move(IncludeMacroStack.back().TheTokenLexer);
                    CurLexerCallback = IncludeMacroStack.back().CurLexerCallback;
                    IncludeMacroStack.pop_back();
                }
            }
        }
    }

    /// Callback invoked when the current TokenLexer hits the end of its token stream.
    bool HandleEndOfTokenLexer(Token &Result);
    {
        // Handle this like a #include file being popped off the stack.
        return HandleEndOfFile(Result, true);
    }

    /// Lex the next token for this preprocessor.
    void Lex(Token &Result);
    {
        CurLexerCallback(*this, Result);
        {
            CurLexer->Lex(Result);
        }
    }

    // callbacks: lexer在处理过程中调用callback触发预处理

    /// Callback invoked when the lexer reads an identifier and has
    /// filled in the tokens IdentifierInfo member.
    /// 检查已识别的标识符是否是已定义的macro
    bool HandleIdentifier(Token &Identifier);

    /// Callback invoked when the lexer sees a # token at the start of a line.
    ///
    /// This consumes the directive, modifies the lexer/preprocessor state, and
    /// advances the lexer(s) so that the next token read is the correct one.
    void HandleDirective(Token &Result);

    // 处理各种macro指令
    void HandleIncludeDirective(...);
    void HandleDefineDirective(...);
    void HandleUndefDirective();
    void HandleIfdefDirective(...);
    void HandleIfDirective(...);
    void HandleEndifDirective(...);
    void HandleElseDirective(...);
    void HandleElifFamilyDirective(...);
    void HandlePragmaDirective(...);
    void HandleUndefDirective(...);
}

/// Lexer - This provides a simple interface that turns a text buffer into a stream of tokens.
/// Lexers know only about tokens within a single source file, and don't
/// know anything about preprocessor-level issues like the \#include stack, etc.
/// lexer只能看到某一个源文件，看不到全局
class Lexer : public PreprocessorLexer {
    // Start of the buffer.
    const char *BufferStart;
    // End of the buffer.
    const char *BufferEnd;
    // BufferPtr - Current pointer into the buffer. This is the next character to be lexed.
    const char *BufferPtr;

    void InitLexer(const char *BufStart, const char *BufPtr, const char *BufEnd);

    /// Lex - Return the next token in the file.
    /// If this is the end of file, it return the tok::eof token.
    bool Lex(Token &Result);
}

/// TokenLexer - This implements a lexer that returns tokens from a macro body
/// or token stream instead of lexing from a character buffer.  This is used for
/// macro expansion and _Pragma handling, for example.
class TokenLexer {

    /// The macro we are expanding from. This is null if expanding a token stream.
    MacroInfo *Macro = nullptr;

    /// The actual arguments specified for a function-like macro, or null.
    MacroArgs *ActualArgs = nullptr;

    /// This is the pointer to an array of tokens that the macro is defined to,
    /// with arguments expanded for function-like macros. 
    /// If this is a token stream, these are the tokens we are returning.
    const Token *Tokens;

public:
    // 从宏扩展的结果提取token
    TokenLexer(Token &Tok, ..., MacroInfo *MI, MacroArgs *ActualArgs, Preprocessor &pp) {
        Macro = MI;
        ActualArgs = Actuals;
        Tokens = &*Macro->tokens_begin();
    }

    // 直接从token流提取token
    TokenLexer(const Token *TokArray, unsigned NumToks, ..., Preprocessor &pp) {
        Macro = nullptr;
        ActualArgs = nullptr;
        Tokens = TokArray;
        NumTokens = NumToks;
        CurTokenIdx = 0;
    }

    /// Lex and return a token from this macro stream.
    bool Lex(Token &Tok);
}

/// Sema - This implements semantic analysis and AST building for C.
class Sema final {
public:
    Preprocessor &PP;

    // 全局声明符号表
    ASTContext &Context;

    // AST的使用方
    ASTConsumer &Consumer;

    // 当前的作用域: 链表结构
    /// The parser's current scope.
    Scope *CurScope;

    /// CurContext - 当前作用域内的声明
    DeclContext *CurContext;

    void Initialize();

    // 管理作用域
    void PushFunctionScope();
    void PushBlockScope(Scope *BlockScope, BlockDecl *Block);
    void PushCompoundScope(bool IsStmtExpr);
    void PopCompoundScope();
    void ActOnPopScope(SourceLocation Loc, Scope *S);
    void ActOnTranslationUnitScope(Scope *S);

    void PushDeclContext(Scope *S, DeclContext *DC);
    void PopDeclContext();
    void EnterDeclaratorContext(Scope *S, DeclContext *DC);
    void ExitDeclaratorContext(Scope *S);

    // 维护自身状态
    void ActOnStartOfTranslationUnit();
    void ActOnEndOfTranslationUnit();
    void ActOnStartFunctionDeclarationDeclarator(Declarator &D, ...);
    void ActOnFinishFunctionDeclarationDeclarator(Declarator &D);
    void ActOnFinishInlineFunctionDef(FunctionDecl *D);
    void ActOnDefinedDeclarationSpecifier(Decl *D);
    void ActOnDefs(Scope *S, Decl *TagD, ...);
    void ActOnEnumBody(...);
    void ActOnStartOfCompoundStmt(bool IsStmtExpr);
    void ActOnFinishOfCompoundStmt();
    void ActOnForEachDeclStmt(DeclGroupPtrTy Decl);
    void ActOnCaseStmtBody(Stmt *CaseStmt, Stmt *SubStmt);

    // 生成ast
    Decl *ActOnDeclarator(Scope *S, Declarator &D);
    Decl *ActOnParamDeclarator(Scope *S, Declarator &D, ...);
    Decl *ActOnStartOfFunctionDef(Scope *S, Declarator &D, ...);
    Decl *ActOnFinishFunctionBody(Decl *Decl, Stmt *Body);
    Decl *ActOnFileScopeAsmDecl(Expr *expr, ...);
    Decl *ActOnTopLevelStmtDecl(Stmt *Statement);
    Decl *ActOnField(Scope *S, Decl *TagD, ...);
    Decl *ActOnEnumConstant(Scope *S, Decl *EnumDecl, Decl *LastEnumConstant, ...);

    TypeResult ActOnTypeName(Declarator &D);
    NamedDecl* ActOnFunctionDeclarator(Scope* S, Declarator& D, ...);

    StmtResult ActOnExprStmt(ExprResult Arg, bool DiscardedValue = true);
    StmtResult ActOnCompoundStmt(...);
    StmtResult ActOnDeclStmt(DeclGroupPtrTy Decl, ...);
    StmtResult ActOnForEachLValueExpr(Expr *E);
    StmtResult ActOnCaseStmt(ExprResult LHS, ...);
    StmtResult ActOnDefaultStmt(...);
    StmtResult ActOnLabelStmt(...);
    StmtResult ActOnIfStmt(...);
    StmtResult ActOnStartOfSwitchStmt(...);
    StmtResult ActOnFinishSwitchStmt(...);
    StmtResult ActOnWhileStmt(...);
    StmtResult ActOnDoStmt(...);
    StmtResult ActOnForStmt(...);
    StmtResult ActOnGotoStmt(...);
    StmtResult ActOnIndirectGotoStmt(...);
    StmtResult ActOnContinueStmt(...);
    StmtResult ActOnBreakStmt(...);
    StmtResult ActOnReturnStmt(...);
    StmtResult ActOnGCCAsmStmt(...);

    ExprResult ActOnPredefinedExpr(...);
    ExprResult ActOnIntegerConstant(...);
    ExprResult ActOnNumericConstant(...);
    ExprResult ActOnCharacterConstant(...);
    ExprResult ActOnParenExpr(...);
    ExprResult ActOnParenListExpr(...);
    ExprResult ActOnStringLiteral(...);
    ExprResult ActOnUnaryOp(...);
    ExprResult ActOnUnaryExprOrTypeTraitExpr(...);
    ExprResult ActOnPostfixUnaryOp(...);
    ExprResult ActOnArraySubscriptExpr(...);
    ExprResult ActOnMemberAccessExpr(...);
    ExprResult ActOnCallExpr(...);
    ExprResult ActOnCastExpr(...);
    ExprResult ActOnCaseExpr(..., ExprResult Val);
    ExprResult ActOnBinOp(...);
    ExprResult ActOnConditionalOp(...);
    ExprResult ActOnStmtExpr(...);
    ExprResult ActOnStmtExprResult(...);
}

```

![ast](/assets/images/2024-05-24/ast.png)

从以上核心代码可以看出他们的职责和关系如下：
* `Lexer`负责解析单个源文件的`token`，`TokenLexer`负责处理`macro`扩展
* `Preprocessor`负责管理`lexer`栈并处理`#include/#define`等预处理指令
* `Parser`负责根据语法规则驱动解析的过程，从`Preprocessor`获取token，把语法单元传递给`Sema`
* `Sema`负责对语法单元进行语义分析以及生成`AST`


## 词法分析

`Lexer`负责解析单个源文件的`token`，遇到标识符和宏指令时通过`callback`通知`Preprocessor`进行处理

```c++

// 解析每个字符，根据c11的词法规则分析其类型
bool Lexer::Lex(Token &Result) {
    // Start a new token.
    Result.startToken();
    {
        Kind = tok::unknown;
        Flags = 0;
        PtrData = nullptr;
        UintData = 0;
    }

    return LexTokenInternal(Result, atPhysicalStartOfLine);
    {
    LexStart:
        const char *CurPtr = BufferPtr;

        // 跳过空白
        if (isHorizontalWhitespace(*CurPtr)) {
            do {
                ++CurPtr;
            } while (isHorizontalWhitespace(*CurPtr));

            BufferPtr = CurPtr;
            Result.setFlag(Token::LeadingSpace);
        }

        // 处理转义，获取一个有效字符
        char Char = getAndAdvanceChar(CurPtr, Result);

        tok::TokenKind Kind;

        // 数字常量/字符串常量/标识符/宏指令/各种标点及其组合
        switch (Char) {
        case 0:  // Null.
            // Found end of file?
            if (CurPtr - 1 == BufferEnd)
                return LexEndOfFile(Result, CurPtr-1);
            
            SkipWhitespace(Result, CurPtr, TokAtPhysicalStartOfLine);
            goto LexNextToken;
        case '\r':
            if (CurPtr[0] == '\n')
                (void)getAndAdvanceChar(CurPtr, Result);
            [[fallthrough]];
        case '\n':
            // No leading whitespace seen so far.
            Result.clearFlag(Token::LeadingSpace);
            SkipWhitespace(Result, CurPtr, TokAtPhysicalStartOfLine);
            goto LexNextToken;
        case ' ':
        case '\t':
            // 跳过行内空白
            SkipHorizontalWhitespace:
                Result.setFlag(Token::LeadingSpace);
                SkipWhitespace(Result, CurPtr, TokAtPhysicalStartOfLine);
            
            // 跳过注释
            SkipIgnoredUnits:
                CurPtr = BufferPtr;

            if (CurPtr[0] == '/' && CurPtr[1] == '/') {
                SkipLineComment(Result, CurPtr+2, TokAtPhysicalStartOfLine);
                goto SkipIgnoredUnits;
            } else if (CurPtr[0] == '/' && CurPtr[1] == '*') {
                SkipBlockComment(Result, CurPtr+2, TokAtPhysicalStartOfLine);
                goto SkipIgnoredUnits;
            } else if (isHorizontalWhitespace(*CurPtr)) {
                goto SkipHorizontalWhitespace;
            }

            goto LexNextToken;
        
        // 数字常量
        case '0': case '1': case '2': case '3': case '4':
        case '5': case '6': case '7': case '8': case '9':
            return LexNumericConstant(Result, CurPtr);
        
        // Identifier (e.g., uber), or
        // UTF-8 (C23/C++17) or UTF-16 (C11/C++11) character literal, or
        // UTF-8 or UTF-16 string literal (C11/C++11).
        case 'u':
        // Identifier (e.g. Uber) or C11/C++11 UTF-32 string literal
        case 'U':
        // Identifier or C++0x raw string literal
        case 'R':
            // 对于c11来说都是普通字符
            return LexIdentifierContinue(Result, CurPtr);
        // Identifier (e.g. Loony) or wide literal (L'x' or L"xyz").
        case 'L':
            Char = getCharAndSize(CurPtr, SizeTmp);
            // Wide string literal.
            if (Char == '"')
                return LexStringLiteral(Result, ...);
            // Wide character constant.
            if (Char == '\'')
                return LexCharConstant(Result, ...);
            [[fallthrough]];
        // C99 6.4.2: Identifiers.
        case 'A': case 'B': case 'C': case 'D': case 'E': case 'F': case 'G':
        case 'H': case 'I': case 'J': case 'K':    /*'L'*/case 'M': case 'N':
        case 'O': case 'P': case 'Q':    /*'R'*/case 'S': case 'T':    /*'U'*/
        case 'V': case 'W': case 'X': case 'Y': case 'Z':
        case 'a': case 'b': case 'c': case 'd': case 'e': case 'f': case 'g':
        case 'h': case 'i': case 'j': case 'k': case 'l': case 'm': case 'n':
        case 'o': case 'p': case 'q': case 'r': case 's': case 't':    /*'u'*/
        case 'v': case 'w': case 'x': case 'y': case 'z':
        case '_':
            return LexIdentifierContinue(Result, CurPtr);
        // C99 6.4.4: Character Constants.
        case '\'':
            return LexCharConstant(Result, CurPtr, tok::char_constant);
        // C99 6.4.5: String Literals.
        case '"':
            return LexStringLiteral(Result, CurPtr, ...);
        case '$':
            Kind = tok::unknown;
            break;   
        // C99 6.4.6: Punctuators.
        case '?':
            Kind = tok::question;
            break;
        case '[':
            Kind = tok::l_square;
            break;
        case ']':
            Kind = tok::r_square;
            break;
        case '(':
            Kind = tok::l_paren;
            break;
        case ')':
            Kind = tok::r_paren;
            break;
        case '{':
            Kind = tok::l_brace;
            break;
        case '}':
            Kind = tok::r_brace;
            break;
        case '.':
            Char = getCharAndSize(CurPtr, SizeTmp);
            // 浮点数
            if (Char >= '0' && Char <= '9') {
                return LexNumericConstant(Result, ...);
            } else if (Char == '.' && getCharAndSize(CurPtr+SizeTmp, SizeTmp2) == '.') {
                // 三个点
                Kind = tok::ellipsis;
                CurPtr = ConsumeChar(ConsumeChar(CurPtr, SizeTmp, Result), SizeTmp2, Result);
            } else {
                Kind = tok::period;
            }
            break;
        case '&':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '&') {
                Kind = tok::ampamp;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else if (Char == '=') {
                Kind = tok::ampequal;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else {
                Kind = tok::amp;
            }
            break;
        case '*':
            if (getCharAndSize(CurPtr, SizeTmp) == '=') {
                Kind = tok::starequal;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else {
                Kind = tok::star;
            }
            break;
        case '+':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '+') {
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::plusplus;
            } else if (Char == '=') {
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::plusequal;
            } else {
                Kind = tok::plus;
            }
            break;
        case '-':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '-') {      // --
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::minusminus;
            } else if (Char == '>') {   // ->
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::arrow;
            } else if (Char == '=') {   // -=
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::minusequal;
            } else {
                Kind = tok::minus;
            }
            break;
        case '~':
            Kind = tok::tilde;
            break;
        case '!':
            if (getCharAndSize(CurPtr, SizeTmp) == '=') {
                Kind = tok::exclaimequal;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else {
                Kind = tok::exclaim;
            }
            break;
        case '/':
            // 6.4.9: Comments
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '/') {         // Line comment.
                SkipLineComment(Result, ...);
                goto SkipIgnoredUnits;
            }
            if (Char == '*') {  // /**/ comment.
                SkipBlockComment(Result, ...);
                goto LexNextToken;
            }

            if (Char == '=') {
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::slashequal;
            } else {
                Kind = tok::slash;
            }
            break;
        case '%':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '=') {
                Kind = tok::percentequal;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else {
                Kind = tok::percent;
            }
            break;
        case '<':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (ParsingFilename) {
                return LexAngledStringLiteral(Result, CurPtr);
            } else if (Char == '<') {
                char After = getCharAndSize(CurPtr+SizeTmp, SizeTmp2);
                if (After == '=') {
                    Kind = tok::lesslessequal;
                    CurPtr = ConsumeChar(ConsumeChar(CurPtr, SizeTmp, Result), SizeTmp2, Result);
                } else {
                    CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                    Kind = tok::lessless;
                }
            } else if (Char == '=') {
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::lessequal;
            } else {
                Kind = tok::less;
            }
            break;
        case '>':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '=') {
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::greaterequal;
            } else if (Char == '>') {
                char After = getCharAndSize(CurPtr+SizeTmp, SizeTmp2);
                if (After == '=') {
                    CurPtr = ConsumeChar(ConsumeChar(CurPtr, SizeTmp, Result), SizeTmp2, Result);
                    Kind = tok::greatergreaterequal;
                } else {
                    CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                    Kind = tok::greatergreater;
                }
            } else {
                Kind = tok::greater;
            }
            break;
        case '^':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '=') {
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
                Kind = tok::caretequal;
            } else {
                Kind = tok::caret;
            }
            break;
        case '|':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '=') {
                Kind = tok::pipeequal;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else if (Char == '|') {
                Kind = tok::pipepipe;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else {
                Kind = tok::pipe;
            }
            break;
        case ':':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == ':') {
                Kind = tok::coloncolon;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else {
                Kind = tok::colon;
            }
            break;
        case ';':
            Kind = tok::semi;
            break;
        case '=':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '=') {
                Kind = tok::equalequal;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else {
                Kind = tok::equal;
            }
            break;
        case ',':
            Kind = tok::comma;
            break;
        case '#':
            Char = getCharAndSize(CurPtr, SizeTmp);
            if (Char == '#') {
                Kind = tok::hashhash;
                CurPtr = ConsumeChar(CurPtr, SizeTmp, Result);
            } else {
                // We parsed a # character.  If this occurs at the start of the line,
                // it's actually the start of a preprocessing directive.  Callback to
                // the preprocessor to handle it.
                if (TokAtPhysicalStartOfLine && !LexingRawMode && !Is_PragmaLexer)
                    goto HandleDirective;

                Kind = tok::hash;
            }
            break;
        case '@':
            Kind = tok::unknown;
            break;

        // UCNs (C99 6.4.3, C++11 [lex.charset]p2)
        case '\\':
            if (uint32_t CodePoint = tryReadUCN(CurPtr, BufferPtr, &Result)) {
                if (CheckUnicodeWhitespace(Result, CodePoint, CurPtr)) {
                    SkipWhitespace(Result, CurPtr, TokAtPhysicalStartOfLine);

                    // We only saw whitespace, so just try again with this lexer.
                    // (We manually eliminate the tail call to avoid recursion.)
                    goto LexNextToken;
                }

                return LexUnicodeIdentifierStart(Result, CodePoint, CurPtr);
            }

            Kind = tok::unknown;
            break;

        default: {
            if (isASCII(Char)) {
                Kind = tok::unknown;
                break;
            }

            BufferPtr = CurPtr+1;
            goto LexNextToken;
        }
        }

        // Update the location of token as well as BufferPtr.
        FormTokenWithChars(Result, CurPtr, Kind);
        return true;

    HandleDirective:
        // 处理#开头的指令
        FormTokenWithChars(Result, CurPtr, tok::hash);
        PP->HandleDirective(Result);
        return false;

    LexNextToken:
        Result.clearFlag(Token::NeedsCleaning);
        goto LexStart;
    }
}

// 识别标识符
bool Lexer::LexIdentifierContinue(Token &Result, const char *CurPtr) {
    CurPtr = fastParseASCIIIdentifier(CurPtr, BufferEnd);
    {
        unsigned char C = *CurPtr;
        while (isAsciiIdentifierContinue(C))
            C = *++CurPtr;
        return CurPtr;
    }

    const char *IdStart = BufferPtr;
    FormTokenWithChars(Result, CurPtr, tok::raw_identifier);
    Result.setRawIdentifierData(IdStart);

    const IdentifierInfo *II = PP->LookUpIdentifierInfo(Result);

    // 判断标识符是否是已定义的macro，是则需要进行宏扩展
    // Finally, now that we know we have an identifier, pass this off to the
    // preprocessor, which may macro expand it or something.
    if (II->isHandleIdentifierCase())
        return PP->HandleIdentifier(Result);

    return true;
}

```

`TokenLexer`负责处理宏扩展

```c++

bool TokenLexer::Lex(Token &Tok) {
    if (isAtEnd()) {
        // 根据c11规则: 正在扩展的macro不允许再次扩展
        if (Macro) Macro->EnableMacro();

        Tok.startToken();
        Tok.setFlagValue(Token::StartOfLine , AtStartOfLine);
        Tok.setFlagValue(Token::LeadingSpace, ...);
        if (CurTokenIdx == 0)
            Tok.setFlag(Token::LeadingEmptyMacro);
        return PP.HandleEndOfTokenLexer(Tok);
    }

    bool isFirstToken = CurTokenIdx == 0;

    // Get the next token to return.
    Tok = Tokens[CurTokenIdx++];

    // 宏扩展时遇到##表示其前后的token需要进行拼接
    // If this token is followed by a token paste (##) operator, paste the tokens!
    if (!isAtEnd() && Macro && Tokens[CurTokenIdx].is(tok::hashhash)) {
        pasteTokens(Tok);
        {
            // 循环处理连续的##: a##b##c
            do {
                const Token &LHSTok = Tok;
                const Token &RHSTok = TokenStream[CurIdx];

                SmallString<128> Buffer;
                Buffer.resize(LHSTok.getLength() + RHSTok.getLength());

                // 在buffer中合并左右token
                unsigned LHSLen = PP.getSpelling(LHSTok, BufPtr, &Invalid);
                memcpy(&Buffer[0], BufPtr, LHSLen);
                unsigned RHSLen = PP.getSpelling(RHS, BufPtr, &Invalid);
                memcpy(&Buffer[LHSLen], BufPtr, RHSLen);

                // 根据合并后的字符串创建token
                Token ResultTokTmp;
                ResultTokTmp.startToken();
                ResultTokTmp.setKind(tok::string_literal);
                PP.CreateString(Buffer, ResultTokTmp);
                ResultTokStrPtr = ResultTokTmp.getLiteralData();

                Token Result;

                if (LHSTok.isAnyIdentifier() && RHSTok.isAnyIdentifier()) {
                    // 两个标识符合并完肯定还是标识符
                    Result.startToken();
                    Result.setKind(tok::raw_identifier);
                    Result.setRawIdentifierData(ResultTokStrPtr);
                    Result.setLength(LHSLen+RHSLen);
                } else {
                    // 创建lexer处理合并之后的字符串
                    Lexer TL(..., ResultTokStrPtr, ResultTokStrPtr+LHSLen+RHSLen);
                    {
                        // We *are* in raw mode.
                        LexingRawMode = true;
                    }

                    // 合并之后的字符串必须是一个token
                    // 这就是下方connect2有时会报错的原因
                    bool isInvalid = !TL.LexFromRawLexer(Result);
                    if (isInvalid) {
                        break;
                    }

                    // `# ## #`之后的结果不再具有特殊含义
                    if (Result.is(tok::hashhash))
                        Result.setKind(tok::unknown);
                }
                
                // Finally, replace LHS with the result, consume the RHS, and iterate.
                ++CurIdx;
                LHSTok = Result;
            } while (!IsAtEnd() && TokenStream[CurIdx].is(tok::hashhash));
        }
    }

    if (isFirstToken) {
        Tok.setFlagValue(Token::StartOfLine , AtStartOfLine);
        Tok.setFlagValue(Token::LeadingSpace, HasLeadingSpace);
    } else {
        if (AtStartOfLine) Tok.setFlag(Token::StartOfLine);
        if (HasLeadingSpace) Tok.setFlag(Token::LeadingSpace);
    }
    AtStartOfLine = false;
    HasLeadingSpace = false;

    // 扩展过程中又遇到已定义的宏需要扩展
    // Handle recursive expansion!
    if (Tok.getIdentifierInfo() != nullptr) {
        IdentifierInfo *II = Tok.getIdentifierInfo();
        Tok.setKind(II->getTokenID());

        if (!DisableMacroExpansion && II->isHandleIdentifierCase())
            return PP.HandleIdentifier(Tok);
    }

    return true;
}

```

## 预处理

macro扩展过程如下：

```c++

// 检查标识符是否需要扩展
void Preprocessor::HandleIdentifier(Token & Identifier) {
    IdentifierInfo &II = *Identifier.getIdentifierInfo();

    // 检查macro符号表判断标识符是否是已定义的macro
    if (const MacroDefinition MD = getMacroDefinition(&II)) {
        const auto *MI = MD.getMacroInfo();

        if (!Identifier.isExpandDisabled() && MI->isEnabled()) {
            // C99 6.10.3p10: If the preprocessing token immediately after the
            // macro name isn't a '(', this macro should not be expanded.
            // 简单宏和函数宏都要扩展
            if (MI->isObjectLike() || isNextPPTokenLParen())
                return HandleMacroExpandedIdentifier(Identifier, MD);
        } else {
            // C99 6.10.3.4p2 says that a disabled macro may never again be
            // expanded, even if it's in a context where it could be expanded in the future.
            // 一个宏在其某次递归扩展的过程中只能扩展一次
            Identifier.setFlag(Token::DisableExpand);
        }
    }

    return true;
}

/// HandleMacroExpandedIdentifier - If an identifier token is read that is to be
/// expanded as a macro, handle it and return the next token as 'Identifier'.
bool Preprocessor::HandleMacroExpandedIdentifier(Token &Identifier,
                                                 const MacroDefinition &M) {
    MacroInfo *MI = M.getMacroInfo();

    // If this is a builtin macro, like __LINE__ or _Pragma, handle it specially.
    if (MI->isBuiltinMacro()) {
        if (Callbacks)
            Callbacks->MacroExpands(Identifier, M, ..., /*Args=*/nullptr);
        ExpandBuiltinMacro(Identifier);
        return true;
    }

    /// Args - If this is a function-like macro expansion, this contains,
    /// for each macro argument, the list of tokens that were provided to the invocation.
    MacroArgs *Args = nullptr;

    // If this is a function-like macro, read the arguments.
    if (MI->isFunctionLike()) {
        // 解析macro扩展的参数
        Args = ReadMacroCallArgumentList(Identifier, MI, ExpansionEnd);
        {
            // The number of fixed arguments to parse.
            unsigned NumFixedArgsLeft = MI->getNumParams();
            bool isVariadic = MI->isVariadic();

            LexUnexpandedToken(Tok);

            SmallVector<Token, 64> ArgTokens;

            // macro实参不是简单的按逗号分割，还要考虑小括号
            unsigned NumActuals = 0;
            while (Tok.isNot(tok::r_paren)) {
                assert(Tok.isOneOf(tok::l_paren, tok::comma), ...);

                // 解析一个参数，可能参数自身带小括号
                while (true) {
                    LexUnexpandedToken(Tok);

                    if (Tok.is(tok::r_paren)) {
                        // 左右括号配对视为一个参数
                        if (NumParens-- == 0) {
                            break;
                        }
                    } else if (Tok.is(tok::l_paren)) {
                        ++NumParens;
                    } 

                    ArgTokens.push_back(Tok);
                }

                // 每个macro参数值可能包含多个token，插入EOFTok作为分割的标记
                Token EOFTok;
                EOFTok.startToken();
                EOFTok.setKind(tok::eof);
                EOFTok.setLength(0);
                ArgTokens.push_back(EOFTok);
            }

            // 处理可变参数参数

            return MacroArgs::create(MI, ArgTokens, isVarargsElided, *this);
        }
    }

    // 对于`#define VAL 42`这种情况直接扩展
    if (MI->getNumTokens() == 1 &&
        isTrivialSingleTokenExpansion(MI, Identifier.getIdentifierInfo(), *this)) {
        // 直接替换
        Identifier = MI->getReplacementToken(0);

        // If this is a disabled macro or #define X X, we must mark the result as unexpandable.
        if (IdentifierInfo *NewII = Identifier.getIdentifierInfo()) {
            if (MacroInfo *NewMI = getMacroInfo(NewII))
                // 对于`#define X X`停止扩展
                if (!NewMI->isEnabled() || NewMI == MI) {
                    Identifier.setFlag(Token::DisableExpand);
                }
        }
    }

    // Start expanding the macro.
    EnterMacro(Identifier, ExpansionEnd, MI, Args);
    {
        TokLexer->Init(Tok, ILEnd, Macro, Args);
        {
            Macro = MI;
            ActualArgs = Actuals;
            NumTokens = Macro->tokens_end() - Macro->tokens_begin();

            if (Macro->isFunctionLike() && Macro->getNumParams())
                // 提前扩展函数宏
                ExpandFunctionArguments();
                {
                    SmallVector<Token, 128> ResultToks;

                    // 作为结果的ResultToks当作字符串再次进行预处理，
                    // 以此递归遍历body的每个token
                    for (unsigned I = 0, E = NumTokens; I != E; ++I) {
                        const Token &CurTok = Tokens[I];

                        // 如果body中出现#
                        if (CurTok.isOneOf(tok::hash)) {
                            // 获取下一个token对应的参数的位置
                            int ArgNo = Macro->getParameterNum(Tokens[I+1].getIdentifierInfo());
                            assert(ArgNo != -1, ...);

                            // 获取未扩展的参数值
                            const Token *UnexpArg = ActualArgs->getUnexpArgument(ArgNo);
                            // 参数值字符串化之后作为一个token: 两边添加双引号
                            Token Res = MacroArgs::StringifyArgument(UnexpArg, ...);
                            {
                                // Stringify all the tokens.
                                SmallString<128> Result;
                                Result += "\"";
                                // 拼接其他字符
                                ...
                                Result += '"';
                                PP.CreateString(Result, Tok, ...);
                                return Tok;
                            }
                            // 插入扩展结果
                            ResultToks.push_back(Res);
                            continue;
                        }

                        // 前一个token是##
                        bool PasteBefore = I != 0 && Tokens[I-1].is(tok::hashhash);
                        // 后一个token是##
                        bool PasteAfter = I+1 != E && Tokens[I+1].is(tok::hashhash);

                        IdentifierInfo *II = CurTok.getIdentifierInfo();
                        int ArgNo = II ? Macro->getParameterNum(II) : -1;
                        if (ArgNo == -1) {
                            // 非宏参数保持不变
                            ResultToks.push_back(CurTok);
                            continue;
                        }

                        // 是宏参数且前后都没有##
                        if (!PasteBefore && !PasteAfter) {
                            // 获取未扩展的参数值
                            const Token *ArgTok = ActualArgs->getUnexpArgument(ArgNo);
                            // 参数值包含macro定义则递归扩展
                            if (ActualArgs->ArgNeedsPreexpansion(ArgTok, PP))
                                // 对参数值进行宏扩展
                                ResultArgToks = &ActualArgs->getPreExpArgument(ArgNo, PP)[0];
                            else
                                ResultArgToks = ArgTok; 

                            if (ResultArgToks->isNot(tok::eof)) {
                                // 记录当前位置
                                size_t FirstResult = ResultToks.size();
                                // 插入扩展之后的token
                                unsigned NumToks = MacroArgs::getArgLength(ResultArgToks);
                                ResultToks.append(ResultArgToks, ResultArgToks+NumToks);

                                // 刚插入的token如果有##则标记忽略
                                for (Token &Tok : llvm::drop_begin(ResultToks, FirstResult))
                                    if (Tok.is(tok::hashhash))
                                        Tok.setKind(tok::unknown);

                                continue;
                            }
                        }

                        const Token *ArgToks = ActualArgs->getUnexpArgument(ArgNo);
                        unsigned NumToks = MacroArgs::getArgLength(ArgToks);
                        if (NumToks) { 
                            // 前后有##则不扩展也不字符串化原样插入
                            ResultToks.append(ArgToks, ArgToks+NumToks);

                            // 刚插入的token如果有##则标记忽略
                            for (Token &Tok : llvm::make_range(ResultToks.end() - NumToks,
                                                               ResultToks.end()))
                                if (Tok.is(tok::hashhash))
                                    Tok.setKind(tok::unknown);

                            continue;
                        }
                    }
                }

            // Mark the macro as currently disabled, so that it is not recursively expanded. 
            // The macro must be disabled only after argument pre-expansion of
            // function-like macro arguments occurs.
            // c语言规范: 标记当前扩展的宏在其本次递归扩展过程中不会被再次扩展
            Macro->DisableMacro();
        }

        // 保存当前的lexer
        PushIncludeMacroStack();
        CurTokenLexer = std::move(TokLexer);
        CurLexerCallback = CLK_TokenLexer;
    }

    return false;
}

```

以下示例再次解释macro扩展的算法

```c

#define AA 11 
#define BB 22 

#define STR(x) #x
#define STR2(x) STR(x)
#define CONNECT(a, b) a##_##b
#define CONNECT2(a, b) CONNECT(a, b)

// macro-body中参数名前带#时其后的参数值整个作为单个字符串
// 字符串的两边自动添加双引号

// "name"
char *p10 = STR(name);
// "#name" 
char *p11 = STR(#name);
// "CONNECT(11, 22)"
char *p20 = STR(CONNECT(11, 22));
// "CONNECT(AA, BB)"
char *p21 = STR(CONNECT(AA, BB));
// "CONNECT2(AA, BB)"
char *p22 = STR(CONNECT2(AA, BB));

// macro-body中参数名前没有#时先扩展参数值，结果插入macro-body
// macro-body中参数名前后有##时，参数值不扩展原样插入macro-body
// 最后得到的macro-body再次进行扩展

// 1. 先扩展参数值: `CONNECT(AA, BB)`
// 1.1 前后有##则表示连接，得到: `AA_BB`
// 2. 把扩展之后的值插入macro-body得到: `STR(AA_BB)`
// 2.1. 是对整个结果继续做扩展得到: `"AA_BB"`
char *p30 = STR2(CONNECT(AA, BB));

// 1. 先扩展参数值: `CONNECT2(AA, BB)`
// 1.1 参数值扩展后插入body，得到: `CONNECT(11, 22)`
// 1.2 前后有##则表示连接，得到: `11_22`
// 2. 参数值插入macro-body，得到: `STR(11_22)`
// 2.1 继续扩展，得到: `"11_22"`
char *p40 = STR2(CONNECT2(AA, BB));

// 1. 先扩展参数值: `CONNECT2(STR2(AA), STR(STR2(BB)))`
// 1.1 扩展参数值: `STR2(AA)`
// 1.1.1 扩展参数值，得到: `11`
// 1.1.2 插入body，得到: `STR(11)`
// 1.1.3 继续扩展，得到: `"11"`
// 1.2 扩展参数值: `STR(STR2(BB))`
// 1.2.1 参数名带#直接转字符串，得到: `"STR2(BB)"`
// 1.3 插入body，得到: `CONNECT("11", "STR2(BB)")`
// 1.3.1 参数名带##直接连接，得到: `"11"_"STR2(BB)"`
// 2  插入body，得到: `STR("11"_"STR2(BB)")`
// 2.1 继续扩展，得到: `""11"_"STR2(BB)""`
// 3 事实上在1.3.1这一步就会报错，原因是##连接的结果必须是单个token
//   很明显`"11"_`不是一个token
char *p50 = STR2(CONNECT2(STR2(AA), STR(STR2(BB))));

```


```c++
/// lexer遇到#开头的宏指令时回调此函数
/// HandleDirective - This callback is invoked when the lexer sees a # token
/// at the start of a line.  This consumes the directive, modifies the
/// lexer/preprocessor state, and advances the lexer(s) so that the next token
/// read is the correct one.
void Preprocessor::HandleDirective(Token &Result) {
    // Read the next token, the directive flavor.  
    // This isn't expanded due to C99 6.10.3p8.
    LexUnexpandedToken(Result);

    switch (Result.getKind()) {
    case tok::numeric_constant:  // # 7  GNU line marker directive.
        return HandleDigitDirective(Result);
    default:
        IdentifierInfo *II = Result.getIdentifierInfo();
        // Ask what the preprocessor keyword ID is.
        switch (II->getPPKeywordID()) {
        default: break;
        // C99 6.10.1 - Conditional Inclusion.
        case tok::pp_if:
            return HandleIfDirective(Result, ...);
        case tok::pp_ifdef:
            return HandleIfdefDirective(Result, ...);
        case tok::pp_ifndef:
            return HandleIfdefDirective(Result, ...);
        case tok::pp_elif:
        case tok::pp_elifdef:
        case tok::pp_elifndef:
            return HandleElifFamilyDirective(Result, ...);
        case tok::pp_else:
            return HandleElseDirective(Result, SavedHash);
        case tok::pp_endif:
            return HandleEndifDirective(Result);

        // C99 6.10.2 - Source File Inclusion.
        case tok::pp_include:
            // Handle #include.
            return HandleIncludeDirective(..., Result);
        case tok::pp___include_macros:
            // Handle -imacros.
            return HandleIncludeMacrosDirective(..., Result);

        // C99 6.10.3 - Macro Replacement.
        case tok::pp_define:
            return HandleDefineDirective(Result, ImmediatelyAfterTopLevelIfndef);
        case tok::pp_undef:
            return HandleUndefDirective();

        // C99 6.10.4 - Line Control.
        case tok::pp_line:
            return HandleLineDirective();

        // C99 6.10.5 - Error Directive.
        case tok::pp_error:
            return HandleUserDiagnosticDirective(Result, false);

        // C99 6.10.6 - Pragma Directive.
        case tok::pp_pragma:
            return HandlePragmaDirective({PIK_HashPragma, SavedHash.getLocation()});
        }
        break;
    }
}

// 处理`#include`
void Preprocessor::HandleIncludeDirective(...) {
    Token FilenameTok;
    LexHeaderName(FilenameTok);

    HandleHeaderIncludeOrImport(FilenameTok, ...);
    {
        OptionalFileEntryRef File = LookupHeaderIncludeOrImport(...);

        // Check that we don't have infinite #include recursion.
        if (IncludeMacroStack.size() == MaxAllowedIncludeStackDepth-1) {
            Diag(FilenameTok, diag::err_pp_include_too_deep);
            HasReachedMaxIncludeDepth = true;
            return {ImportAction::None};
        }

        FileID FID = SourceMgr.createFileID(*File, IncludePos, FileCharacter);

        // 遇到#include之后创建lexer处理被包含的文件
        EnterSourceFile(FID, CurDir, ...);
    }
}

// 处理`#define `
void Preprocessor::HandleDefineDirective(Token &DefineTok, ...) {
    // 解析macro名称
    Token MacroNameTok;
    ReadMacroName(MacroNameTok, MU_Define, ...);

    // 解析参数列表和body
    MacroInfo *const MI = ReadOptionalMacroParameterListAndBody(...);
    {
        // Create the new macro.
        MacroInfo *const MI = AllocateMacroInfo(MacroNameTok.getLocation());

        Token Tok;
        LexUnexpandedToken(Tok);

        if (Tok.is(tok::l_paren)) {
            MI->setIsFunctionLike();
            ReadMacroParameterList(MI, LastTok);

            // Read the first token after the arg list for down below.
            LexUnexpandedToken(Tok);
        } else {
            SmallVector<Token, 16> Tokens;

            // Read the rest of the macro body.
            if (MI->isObjectLike()) {
                // Object-like macros are very simple, just read their body.
                while (Tok.isNot(tok::eod)) {
                    LastTok = Tok;
                    Tokens.push_back(Tok);
                    // Get the next token of the macro.
                    LexUnexpandedToken(Tok);
                }
            } else {
                // 解析函数macro的body
                while (Tok.isNot(tok::eod)) {
                    LastTok = Tok;

                    if (!Tok.isOneOf(tok::hash, tok::hashat, tok::hashhash)) {
                        Tokens.push_back(Tok);
                        LexUnexpandedToken(Tok);
                        continue;
                    }

                    if (Tok.is(tok::hashhash)) {
                        LexUnexpandedToken(Tok);
                        Tokens.push_back(LastTok);
                        continue;
                    }

                    LexUnexpandedToken(Tok);
                    Tokens.push_back(LastTok);
                }
            }
        }

        MI->setTokens(Tokens, BP);
        return MI;
    }

    // 从符号表判断MacroName是否已经定义过
    if (const MacroInfo *OtherMI = getMacroInfo(MacroNameTok.getIdentifierInfo())) {
        // 如果已定义的macro通过#pragma标记为final则发出警告
        if (MacroNameTok.getIdentifierInfo()->isFinal())
            emitFinalMacroWarning(MacroNameTok, /*IsUndef=*/false);
    }

    // appendDefMacroDirective: 注册macro指令
    DefMacroDirective *MD = appendDefMacroDirective(MacroNameTok.getIdentifierInfo(), MI);

    if (Callbacks)
        Callbacks->MacroDefined(MacroNameTok, MD);
}

```


